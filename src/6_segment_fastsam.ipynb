{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11108a48",
   "metadata": {},
   "source": [
    "# FastSAM(Fast Segment Anything Model ) 사용해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7a5357",
   "metadata": {},
   "source": [
    "이미지 내의 모든 객체를 분할하도록 설계"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9055464a",
   "metadata": {},
   "source": [
    "- 공식 문서: https://github.com/CASIA-IVA-Lab/FastSAM?tab=readme-ov-file\n",
    "\n",
    "- 참고 : https://docs.ultralytics.com/ko/models/fast-sam/#track-usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cc17d6",
   "metadata": {},
   "source": [
    "# 1. 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06d217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import FastSAM\n",
    "\n",
    "model = FastSAM(\"../models/FastSAM-s.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7715bd03",
   "metadata": {},
   "source": [
    "# 2. 이미지 예측하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e017433d",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"C:\\Potenup\\DeepLearning-YOLO-Study\\images\\dog.jpeg\"\n",
    "\n",
    "everything_results = model(source, device=\"cpu\", save=True, retina_masks=True, conf=0.8, iou=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b76e07c",
   "metadata": {},
   "source": [
    "# 3. 이미지와 텍스트로 예측하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a90d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model(source, texts=\"a photo of a cap\", save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce49ec5f",
   "metadata": {},
   "source": [
    "# 4. 추가 사용 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24520727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run inference with bboxes prompt\n",
    "# results = model(source, bboxes=[439, 437, 524, 709])\n",
    "\n",
    "# # Run inference with points prompt\n",
    "# results = model(source, points=[[200, 200]], labels=[1])\n",
    "\n",
    "# # Run inference with bboxes and points and texts prompt at the same time\n",
    "# results = model(source, bboxes=[439, 437, 524, 709], points=[[200, 200]], labels=[1], texts=\"a photo of a dog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de62fdc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73e0433",
   "metadata": {},
   "source": [
    "# 5. 공식 문서를 통해 clone 하여 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53673018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Git Clone `git clone https://github.com/CASIA-IVA-Lab/FastSAM.git`\n",
    "# Requirements.txt 설치 \n",
    "# `cd FastSAM`\n",
    "# `uv pip install -r requirements.txt`\n",
    "# CLIP 설치\n",
    "# `uv pip install git+https://github.com/openai/CLIP.git`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfa3f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/CASIA-IVA-Lab/FastSAM?tab=readme-ov-file#model-checkpoints\n",
    "# 필요한 모델을 직접 다운로드 받는다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda38dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python Inference.py --model_path [./models/FastSAM-s.pt : 모델 위치] --img_path [./images/dog.jpeg : 이미지 위치]\n",
    "\n",
    "# cd FastSAM\n",
    "# python Inference.py --model_path [A] --img_path [B]\n",
    "\n",
    "# * python : 파이썬으로 실행해라\n",
    "# * python Inference.py : FastSAM폴더에서 Inference.py를 실행해라\n",
    "# * --model_path, --img_path : 파라미터를 입력하겠다\n",
    "#    * A : FastSAM 폴더에서 model(.pt)의 경로를 입력하세요.\n",
    "#    * B : FastSAM 폴더 안에서 내가 예측할 이미지의 경로를 입력하세요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b81d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 버전 에러가 발생하여 다운 그레이드 한다\n",
    "# uv pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f870a5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning-YOLO-Study (3.10.18)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
